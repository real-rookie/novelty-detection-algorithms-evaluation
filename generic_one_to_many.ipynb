{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/real-rookie/novelty-detection-algorithms-evaluation/blob/main/generic_one_to_many.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEar3q8DHcZI"
      },
      "outputs": [],
      "source": [
        "# env\n",
        "!pip install lightning\n",
        "!pip install anomalib\n",
        "!pip install OpenVINO\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkEZzi7WLsMj"
      },
      "outputs": [],
      "source": [
        "# unzip code and datasets\n",
        "!unzip -o /content/drive/MyDrive/novelty-detection-algorithms-evaluation.zip -d /home/\n",
        "%cd /home"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkWjFmPfIM1T"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar1IxM5TNDEh"
      },
      "outputs": [],
      "source": [
        "# set parameters\n",
        "DATASET_INFO = {\n",
        "    # idx 0: paths, idx 1: number of classes\n",
        "    \"MNIST\": [\"datasets/MNIST/images\", 10],\n",
        "    \"FashionMNIST\": [\"datasets/FashionMNIST/images\", 10],\n",
        "    \"CIFAR10\": [\"datasets/CIFAR10/images\", 10],\n",
        "}\n",
        "dataset = \"MNIST\"\n",
        "dataset_path = DATASET_INFO[dataset][0]\n",
        "num_total_classes = DATASET_INFO[dataset][1]\n",
        "normal_weight = 0.5 # proportion of normal samples in the test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4voDx-W4NEXv",
        "outputId": "bde71e2b-346f-46db-9bcf-c2432663b11e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home\n"
          ]
        }
      ],
      "source": [
        "# make datasets\n",
        "%cd /home\n",
        "os.system(f\"rm -rf {dataset_path}\")\n",
        "for i in range(num_total_classes):\n",
        "    os.system(f\"mkdir -p {dataset_path}/train/{i}\")\n",
        "    os.system(f\"mkdir -p {dataset_path}/categorized_test_cases/{i}\")\n",
        "    os.system(f\"mkdir -p {dataset_path}/test/{i}/normal\")\n",
        "    os.system(f\"mkdir -p {dataset_path}/test/{i}/novel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0bdti_qacfm"
      },
      "outputs": [],
      "source": [
        "train_data = None\n",
        "test_data = None\n",
        "if dataset == \"MNIST\":\n",
        "    train_data = datasets.MNIST(root=\"datasets\", train=True, download=True, transform=ToTensor())\n",
        "    test_data = datasets.MNIST(root=\"datasets\", train=False, download=True, transform=ToTensor())\n",
        "elif dataset == \"FashionMNIST\":\n",
        "    train_data = datasets.FashionMNIST(root=\"datasets\", train=True, download=True, transform=ToTensor())\n",
        "    test_data = datasets.FashionMNIST(root=\"datasets\", train=False, download=True, transform=ToTensor())\n",
        "elif dataset == \"CIFAR10\":\n",
        "    train_data = datasets.CIFAR10(root=\"datasets/CIFAR10\", train=True, download=True, transform=ToTensor())\n",
        "    test_data = datasets.CIFAR10(root=\"datasets/CIFAR10\", train=False, download=True, transform=ToTensor())\n",
        "else:\n",
        "    print(\"Wrong dataset specified\")\n",
        "    os.abort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVKK0aR7Ile1",
        "outputId": "c2320d5d-4930-47d3-969a-ef45cbea530f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n",
            "test: [ 980 1135 1032 1010  982  892  958 1028  974 1009]\n"
          ]
        }
      ],
      "source": [
        "train_counter = np.zeros(num_total_classes, dtype=int)\n",
        "test_counter = np.zeros(num_total_classes, dtype=int)\n",
        "for img, label in train_data:\n",
        "    save_image(img, f\"{dataset_path}/train/{label}/{label}_{train_counter[label]}.png\")\n",
        "    train_counter[label] += 1\n",
        "for img, label in test_data:\n",
        "    save_image(img, f\"{dataset_path}/categorized_test_cases/{label}/{label}_{test_counter[label]}.png\")\n",
        "    test_counter[label] += 1\n",
        "print(f\"train: {train_counter}\")\n",
        "print(f\"test: {test_counter}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aQoYj8RXWdE"
      },
      "outputs": [],
      "source": [
        "def fill_test_sets(population, num_samples, src_cls, dest_cls):\n",
        "    sample_idx = random.sample(range(population), num_samples)\n",
        "    folder_type = \"normal\" if src_cls == dest_cls else \"novel\"\n",
        "    src_set = None\n",
        "    dest_set = None\n",
        "    for index in sample_idx:\n",
        "            os.system(f\"cp {dataset_path}/categorized_test_cases/{src_cls}/{src_cls}_{index}.png {dataset_path}/test/{dest_cls}/{folder_type}\")\n",
        "\n",
        "for normal in range(num_total_classes):\n",
        "    random.seed(normal)\n",
        "\n",
        "    # test normal\n",
        "    num_normal_test_samples = np.floor(test_counter[normal] * normal_weight).astype(int)\n",
        "    fill_test_sets(test_counter[normal], num_normal_test_samples, normal, normal)\n",
        "\n",
        "    num_novel_test_from_each_class = np.floor((test_counter[normal] - num_normal_test_samples) / (num_total_classes - 1)).astype(int)\n",
        "    for novel in range(num_total_classes):\n",
        "        if(novel == normal):\n",
        "            continue\n",
        "        # test novel\n",
        "        fill_test_sets(test_counter[novel], num_novel_test_from_each_class, novel, normal)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0FG2ET9Mp2w",
        "outputId": "4203d0d0-146e-4248-c6a3-4a0f89ad2e11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/novelty-detection-algorithms-evaluation\n",
            "/usr/local/lib/python3.10/dist-packages/anomalib/config/config.py:238: UserWarning: The seed value is now fixed to 0. Up to v0.3.7, the seed was not fixed when the seed value was set to 0. If you want to use the random seed, please select `None` for the seed value (`null` in the YAML file) or remove the `seed` key from the YAML file.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/anomalib/config/config.py:275: UserWarning: config.project.unique_dir is set to False. This does not ensure that your results will be written in an empty directory and you may overwrite files.\n",
            "  warn(\n",
            "Global seed set to 0\n",
            "2023-07-26 23:19:45,920 - anomalib.data - INFO - Loading the datamodule\n",
            "2023-07-26 23:19:45,921 - anomalib.data.utils.transform - INFO - No config file has been provided. Using default transforms.\n",
            "2023-07-26 23:19:45,921 - anomalib.data.utils.transform - INFO - No config file has been provided. Using default transforms.\n",
            "2023-07-26 23:19:45,921 - anomalib.models - INFO - Loading the model.\n",
            "2023-07-26 23:19:45,922 - anomalib.models.components.base.anomaly_module - INFO - Initializing ReverseDistillationLightning model.\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "2023-07-26 23:19:45,929 - anomalib.models.components.feature_extractors.timm - WARNING - FeatureExtractor is deprecated. Use TimmFeatureExtractor instead. Both FeatureExtractor and TimmFeatureExtractor will be removed in a future release.\n",
            "2023-07-26 23:19:47,921 - timm.models.helpers - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)\n",
            "2023-07-26 23:19:48,784 - anomalib.utils.loggers - INFO - Loading the experiment logger(s)\n",
            "2023-07-26 23:19:48,784 - anomalib.utils.callbacks - INFO - Loading the callbacks\n",
            "/usr/local/lib/python3.10/dist-packages/anomalib/utils/callbacks/__init__.py:142: UserWarning: Export option: None not found. Defaulting to no model export\n",
            "  warnings.warn(f\"Export option: {config.optimization.export_mode} not found. Defaulting to no model export\")\n",
            "2023-07-26 23:19:48,804 - pytorch_lightning.utilities.rank_zero - INFO - GPU available: True (cuda), used: True\n",
            "2023-07-26 23:19:48,804 - pytorch_lightning.utilities.rank_zero - INFO - TPU available: False, using: 0 TPU cores\n",
            "2023-07-26 23:19:48,804 - pytorch_lightning.utilities.rank_zero - INFO - IPU available: False, using: 0 IPUs\n",
            "2023-07-26 23:19:48,804 - pytorch_lightning.utilities.rank_zero - INFO - HPU available: False, using: 0 HPUs\n",
            "2023-07-26 23:19:48,804 - pytorch_lightning.utilities.rank_zero - INFO - `Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "2023-07-26 23:19:48,804 - pytorch_lightning.utilities.rank_zero - INFO - `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "2023-07-26 23:19:48,804 - pytorch_lightning.utilities.rank_zero - INFO - `Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "2023-07-26 23:19:48,804 - pytorch_lightning.utilities.rank_zero - INFO - `Trainer(limit_predict_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "2023-07-26 23:19:48,804 - pytorch_lightning.utilities.rank_zero - INFO - `Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "2023-07-26 23:19:48,804 - anomalib - INFO - Training the model.\n",
            "2023-07-26 23:19:49,174 - pytorch_lightning.utilities.rank_zero - INFO - You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `ROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/novelty-detection-algorithms-evaluation/results/RD4AD_MNIST_0/reverse_distillation/MNIST_0/run/weights/lightning exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "2023-07-26 23:19:49,794 - pytorch_lightning.accelerators.cuda - INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "2023-07-26 23:19:49,804 - pytorch_lightning.callbacks.model_summary - INFO - \n",
            "  | Name                  | Type                     | Params\n",
            "-------------------------------------------------------------------\n",
            "0 | image_threshold       | AnomalyScoreThreshold    | 0     \n",
            "1 | pixel_threshold       | AnomalyScoreThreshold    | 0     \n",
            "2 | model                 | ReverseDistillationModel | 80.6 M\n",
            "3 | loss                  | ReverseDistillationLoss  | 0     \n",
            "4 | image_metrics         | AnomalibMetricCollection | 0     \n",
            "5 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
            "6 | normalization_metrics | MinMax                   | 0     \n",
            "-------------------------------------------------------------------\n",
            "80.6 M    Trainable params\n",
            "0         Non-trainable params\n",
            "80.6 M    Total params\n",
            "322.438   Total estimated model params size (MB)\n",
            "Epoch 0:   0% 0/186 [00:00<?, ?it/s] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/module.py:493: UserWarning: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "  rank_zero_warn(\n",
            "Epoch 1: 100% 186/186 [00:23<00:00,  7.98it/s, loss=0.103, train_loss_step=0.118, train_loss_epoch=0.298]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: : 200it [00:24,  8.03it/s, loss=0.103, train_loss_step=0.118, train_loss_epoch=0.298]           \n",
            "Validation DataLoader 0:  65% 20/31 [00:01<00:01, 10.58it/s]\u001b[A\n",
            "Epoch 1: : 200it [00:26,  7.50it/s, loss=0.103, train_loss_step=0.118, train_loss_epoch=0.298, image_AUROC=0.675, image_AUPR=0.577, image_F1Score=0.730]\n",
            "Epoch 3:  83% 180/217 [00:22<00:04,  7.97it/s, loss=0.0724, train_loss_step=0.0764, train_loss_epoch=0.107, image_AUROC=0.675, image_AUPR=0.577, image_F1Score=0.730]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  92% 200/217 [00:24<00:02,  8.04it/s, loss=0.0724, train_loss_step=0.0764, train_loss_epoch=0.107, image_AUROC=0.675, image_AUPR=0.577, image_F1Score=0.730]\n",
            "Validation DataLoader 0:  65% 20/31 [00:01<00:00, 11.07it/s]\u001b[A\n",
            "Epoch 3: 100% 217/217 [00:26<00:00,  8.23it/s, loss=0.074, train_loss_step=0.0847, train_loss_epoch=0.107, image_AUROC=0.229, image_AUPR=0.373, image_F1Score=0.666] \n",
            "Epoch 5:  83% 180/217 [00:22<00:04,  7.95it/s, loss=0.0552, train_loss_step=0.0519, train_loss_epoch=0.0676, image_AUROC=0.229, image_AUPR=0.373, image_F1Score=0.666]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  92% 200/217 [00:24<00:02,  8.03it/s, loss=0.0552, train_loss_step=0.0519, train_loss_epoch=0.0676, image_AUROC=0.229, image_AUPR=0.373, image_F1Score=0.666]\n",
            "Validation DataLoader 0:  65% 20/31 [00:01<00:00, 11.43it/s]\u001b[A\n",
            "Epoch 5: 100% 217/217 [00:26<00:00,  8.22it/s, loss=0.0551, train_loss_step=0.060, train_loss_epoch=0.0676, image_AUROC=0.622, image_AUPR=0.517, image_F1Score=0.734] \n",
            "Epoch 7:  83% 180/217 [00:22<00:04,  7.98it/s, loss=0.0451, train_loss_step=0.0438, train_loss_epoch=0.0519, image_AUROC=0.622, image_AUPR=0.517, image_F1Score=0.734]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  92% 200/217 [00:24<00:02,  8.06it/s, loss=0.0451, train_loss_step=0.0438, train_loss_epoch=0.0519, image_AUROC=0.622, image_AUPR=0.517, image_F1Score=0.734]\n",
            "Validation DataLoader 0:  65% 20/31 [00:01<00:00, 11.16it/s]\u001b[A\n",
            "Epoch 7: 100% 217/217 [00:26<00:00,  8.24it/s, loss=0.0456, train_loss_step=0.0514, train_loss_epoch=0.0519, image_AUROC=0.708, image_AUPR=0.595, image_F1Score=0.750]\n",
            "Epoch 9:  83% 180/217 [00:22<00:04,  7.96it/s, loss=0.0404, train_loss_step=0.0391, train_loss_epoch=0.0442, image_AUROC=0.708, image_AUPR=0.595, image_F1Score=0.750]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  92% 200/217 [00:24<00:02,  8.03it/s, loss=0.0404, train_loss_step=0.0391, train_loss_epoch=0.0442, image_AUROC=0.708, image_AUPR=0.595, image_F1Score=0.750]\n",
            "Validation DataLoader 0:  65% 20/31 [00:01<00:00, 11.38it/s]\u001b[A\n",
            "Epoch 9: 100% 217/217 [00:26<00:00,  8.22it/s, loss=0.042, train_loss_step=0.0788, train_loss_epoch=0.0442, image_AUROC=0.329, image_AUPR=0.403, image_F1Score=0.665] \n",
            "Epoch 11:  83% 180/217 [00:22<00:04,  7.96it/s, loss=0.0378, train_loss_step=0.0402, train_loss_epoch=0.0415, image_AUROC=0.329, image_AUPR=0.403, image_F1Score=0.665]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  92% 200/217 [00:24<00:02,  8.03it/s, loss=0.0378, train_loss_step=0.0402, train_loss_epoch=0.0415, image_AUROC=0.329, image_AUPR=0.403, image_F1Score=0.665]\n",
            "Validation DataLoader 0:  65% 20/31 [00:01<00:00, 11.38it/s]\u001b[A\n",
            "Epoch 11: 100% 217/217 [00:26<00:00,  8.22it/s, loss=0.0384, train_loss_step=0.046, train_loss_epoch=0.0415, image_AUROC=0.776, image_AUPR=0.733, image_F1Score=0.745] \n",
            "Epoch 13:  83% 180/217 [00:22<00:04,  7.94it/s, loss=0.0355, train_loss_step=0.0331, train_loss_epoch=0.0363, image_AUROC=0.776, image_AUPR=0.733, image_F1Score=0.745]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  92% 200/217 [00:24<00:02,  8.02it/s, loss=0.0355, train_loss_step=0.0331, train_loss_epoch=0.0363, image_AUROC=0.776, image_AUPR=0.733, image_F1Score=0.745]\n",
            "Validation DataLoader 0:  65% 20/31 [00:01<00:00, 11.19it/s]\u001b[A\n",
            "Epoch 13: 100% 217/217 [00:26<00:00,  8.21it/s, loss=0.0353, train_loss_step=0.0435, train_loss_epoch=0.0363, image_AUROC=0.795, image_AUPR=0.709, image_F1Score=0.782]\n",
            "Epoch 15:  83% 180/217 [00:22<00:04,  7.97it/s, loss=0.032, train_loss_step=0.0324, train_loss_epoch=0.0337, image_AUROC=0.795, image_AUPR=0.709, image_F1Score=0.782] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  92% 200/217 [00:24<00:02,  8.04it/s, loss=0.032, train_loss_step=0.0324, train_loss_epoch=0.0337, image_AUROC=0.795, image_AUPR=0.709, image_F1Score=0.782]\n",
            "Validation DataLoader 0:  65% 20/31 [00:01<00:00, 11.52it/s]\u001b[A\n",
            "Epoch 15: 100% 217/217 [00:26<00:00,  8.22it/s, loss=0.0344, train_loss_step=0.0713, train_loss_epoch=0.0337, image_AUROC=0.617, image_AUPR=0.594, image_F1Score=0.678]\n",
            "Epoch 17:  83% 180/217 [00:22<00:04,  7.97it/s, loss=0.0303, train_loss_step=0.0325, train_loss_epoch=0.0322, image_AUROC=0.617, image_AUPR=0.594, image_F1Score=0.678]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  92% 200/217 [00:24<00:02,  8.04it/s, loss=0.0303, train_loss_step=0.0325, train_loss_epoch=0.0322, image_AUROC=0.617, image_AUPR=0.594, image_F1Score=0.678]\n",
            "Validation DataLoader 0:  65% 20/31 [00:01<00:00, 11.57it/s]\u001b[A\n",
            "Epoch 17: 100% 217/217 [00:26<00:00,  8.23it/s, loss=0.0308, train_loss_step=0.0409, train_loss_epoch=0.0322, image_AUROC=0.767, image_AUPR=0.657, image_F1Score=0.778]\n",
            "Epoch 19:  83% 180/217 [00:22<00:04,  7.95it/s, loss=0.029, train_loss_step=0.0302, train_loss_epoch=0.0298, image_AUROC=0.767, image_AUPR=0.657, image_F1Score=0.778] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  92% 200/217 [00:24<00:02,  8.02it/s, loss=0.029, train_loss_step=0.0302, train_loss_epoch=0.0298, image_AUROC=0.767, image_AUPR=0.657, image_F1Score=0.778]\n",
            "Validation DataLoader 0:  65% 20/31 [00:01<00:00, 11.48it/s]\u001b[A\n",
            "Epoch 19: 100% 217/217 [00:26<00:00,  8.21it/s, loss=0.0288, train_loss_step=0.030, train_loss_epoch=0.0298, image_AUROC=0.771, image_AUPR=0.693, image_F1Score=0.757]\n",
            "Epoch 19: 100% 217/217 [00:26<00:00,  8.20it/s, loss=0.0288, train_loss_step=0.030, train_loss_epoch=0.0301, image_AUROC=0.771, image_AUPR=0.693, image_F1Score=0.757]\n",
            "2023-07-26 23:28:18,742 - anomalib.utils.callbacks.timer - INFO - Training took 508.95 seconds\n",
            "2023-07-26 23:28:18,742 - anomalib - INFO - Loading the best model weights.\n",
            "2023-07-26 23:28:18,742 - anomalib - INFO - Testing the model.\n",
            "2023-07-26 23:28:18,749 - pytorch_lightning.utilities.rank_zero - INFO - The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: EarlyStopping\n",
            "2023-07-26 23:28:18,750 - pytorch_lightning.utilities.rank_zero - INFO - You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "2023-07-26 23:28:18,750 - anomalib.utils.callbacks.model_loader - INFO - Loading the model from /home/novelty-detection-algorithms-evaluation/results/RD4AD_MNIST_0/reverse_distillation/MNIST_0/run/weights/lightning/model-v1.ckpt\n",
            "2023-07-26 23:28:19,398 - pytorch_lightning.accelerators.cuda - INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing DataLoader 0: 100% 31/31 [00:10<00:00,  3.08it/s]2023-07-26 23:28:30,040 - anomalib.utils.callbacks.timer - INFO - Testing took 10.522866010665894 seconds\n",
            "Throughput (batch_size=32) : 92.75039699362647 FPS\n",
            "Testing DataLoader 0: 100% 31/31 [00:10<00:00,  3.07it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m       image_AUPR        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7085237503051758    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7954522371292114    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7822222113609314    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "/usr/local/lib/python3.10/dist-packages/anomalib/config/config.py:275: UserWarning: config.project.unique_dir is set to False. This does not ensure that your results will be written in an empty directory and you may overwrite files.\n",
            "  warn(\n",
            "Global seed set to 1\n",
            "2023-07-26 23:28:36,461 - anomalib.data - INFO - Loading the datamodule\n",
            "2023-07-26 23:28:36,462 - anomalib.data.utils.transform - INFO - No config file has been provided. Using default transforms.\n",
            "2023-07-26 23:28:36,462 - anomalib.data.utils.transform - INFO - No config file has been provided. Using default transforms.\n",
            "2023-07-26 23:28:36,462 - anomalib.models - INFO - Loading the model.\n",
            "2023-07-26 23:28:36,463 - anomalib.models.components.base.anomaly_module - INFO - Initializing ReverseDistillationLightning model.\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "2023-07-26 23:28:36,470 - anomalib.models.components.feature_extractors.timm - WARNING - FeatureExtractor is deprecated. Use TimmFeatureExtractor instead. Both FeatureExtractor and TimmFeatureExtractor will be removed in a future release.\n",
            "2023-07-26 23:28:38,441 - timm.models.helpers - INFO - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)\n",
            "2023-07-26 23:28:39,294 - anomalib.utils.loggers - INFO - Loading the experiment logger(s)\n",
            "2023-07-26 23:28:39,294 - anomalib.utils.callbacks - INFO - Loading the callbacks\n",
            "/usr/local/lib/python3.10/dist-packages/anomalib/utils/callbacks/__init__.py:142: UserWarning: Export option: None not found. Defaulting to no model export\n",
            "  warnings.warn(f\"Export option: {config.optimization.export_mode} not found. Defaulting to no model export\")\n",
            "2023-07-26 23:28:39,315 - pytorch_lightning.utilities.rank_zero - INFO - GPU available: True (cuda), used: True\n",
            "2023-07-26 23:28:39,315 - pytorch_lightning.utilities.rank_zero - INFO - TPU available: False, using: 0 TPU cores\n",
            "2023-07-26 23:28:39,315 - pytorch_lightning.utilities.rank_zero - INFO - IPU available: False, using: 0 IPUs\n",
            "2023-07-26 23:28:39,315 - pytorch_lightning.utilities.rank_zero - INFO - HPU available: False, using: 0 HPUs\n",
            "2023-07-26 23:28:39,315 - pytorch_lightning.utilities.rank_zero - INFO - `Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "2023-07-26 23:28:39,315 - pytorch_lightning.utilities.rank_zero - INFO - `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "2023-07-26 23:28:39,315 - pytorch_lightning.utilities.rank_zero - INFO - `Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "2023-07-26 23:28:39,315 - pytorch_lightning.utilities.rank_zero - INFO - `Trainer(limit_predict_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "2023-07-26 23:28:39,315 - pytorch_lightning.utilities.rank_zero - INFO - `Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "2023-07-26 23:28:39,315 - anomalib - INFO - Training the model.\n",
            "2023-07-26 23:28:39,678 - pytorch_lightning.utilities.rank_zero - INFO - You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `ROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/novelty-detection-algorithms-evaluation/results/RD4AD_MNIST_1/reverse_distillation/MNIST_1/run/weights/lightning exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "2023-07-26 23:28:40,346 - pytorch_lightning.accelerators.cuda - INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "2023-07-26 23:28:40,356 - pytorch_lightning.callbacks.model_summary - INFO - \n",
            "  | Name                  | Type                     | Params\n",
            "-------------------------------------------------------------------\n",
            "0 | image_threshold       | AnomalyScoreThreshold    | 0     \n",
            "1 | pixel_threshold       | AnomalyScoreThreshold    | 0     \n",
            "2 | model                 | ReverseDistillationModel | 80.6 M\n",
            "3 | loss                  | ReverseDistillationLoss  | 0     \n",
            "4 | image_metrics         | AnomalibMetricCollection | 0     \n",
            "5 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
            "6 | normalization_metrics | MinMax                   | 0     \n",
            "-------------------------------------------------------------------\n",
            "80.6 M    Trainable params\n",
            "0         Non-trainable params\n",
            "80.6 M    Total params\n",
            "322.438   Total estimated model params size (MB)\n",
            "Epoch 0:   0% 0/211 [00:00<?, ?it/s] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/module.py:493: UserWarning: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "  rank_zero_warn(\n",
            "Epoch 0:  95% 200/211 [00:25<00:01,  7.85it/s, loss=0.0878, train_loss_step=0.0823]/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
            "2023-07-26 23:29:06,847 - anomalib - INFO - Loading the best model weights.\n",
            "2023-07-26 23:29:06,848 - anomalib - INFO - Testing the model.\n",
            "2023-07-26 23:29:06,857 - pytorch_lightning.utilities.rank_zero - INFO - The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: EarlyStopping\n",
            "2023-07-26 23:29:06,858 - pytorch_lightning.utilities.rank_zero - INFO - You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "2023-07-26 23:29:06,859 - anomalib.utils.callbacks.model_loader - INFO - Loading the model from \n",
            "Traceback (most recent call last):\n",
            "  File \"/home/novelty-detection-algorithms-evaluation/anomalib/tools/train.py\", line 75, in <module>\n",
            "    train()\n",
            "  File \"/home/novelty-detection-algorithms-evaluation/anomalib/tools/train.py\", line 71, in train\n",
            "    trainer.test(model=model, datamodule=datamodule)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 794, in test\n",
            "    return call._call_and_handle_interrupt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 38, in _call_and_handle_interrupt\n",
            "    return trainer_fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 842, in _test_impl\n",
            "    results = self._run(model, ckpt_path=self.ckpt_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1051, in _run\n",
            "    self._call_setup_hook()  # allow user to setup lightning_module in accelerator environment\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1299, in _call_setup_hook\n",
            "    self._call_callback_hooks(\"setup\", stage=fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1394, in _call_callback_hooks\n",
            "    fn(self, self.lightning_module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anomalib/utils/callbacks/model_loader.py\", line 32, in setup\n",
            "    pl_module.load_state_dict(torch.load(self.weights_path, map_location=pl_module.device)[\"state_dict\"])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 791, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 271, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 252, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: ''\n",
            "Epoch 0:  95%|█████████▍| 200/211 [00:27<00:01,  7.25it/s, loss=0.0878, train_loss_step=0.0823]\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/novelty-detection-algorithms-evaluation/anomalib/tools/train.py\", line 15, in <module>\n",
            "    from pytorch_lightning import Trainer, seed_everything\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/__init__.py\", line 34, in <module>\n",
            "    from lightning_fabric.utilities.seed import seed_everything  # noqa: E402\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning_fabric/__init__.py\", line 23, in <module>\n",
            "    from lightning_fabric.fabric import Fabric  # noqa: E402\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning_fabric/fabric.py\", line 21, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1465, in <module>\n",
            "    from . import _meta_registrations\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 7, in <module>\n",
            "    from torch._decomp import _add_op_to_registry, global_decomposition_table, meta_table\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_decomp/__init__.py\", line 169, in <module>\n",
            "    import torch._decomp.decompositions\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_decomp/decompositions.py\", line 10, in <module>\n",
            "    import torch._prims as prims\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_prims/__init__.py\", line 33, in <module>\n",
            "    from torch._subclasses.fake_tensor import FakeTensor, FakeTensorMode\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/__init__.py\", line 3, in <module>\n",
            "    from torch._subclasses.fake_tensor import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 13, in <module>\n",
            "    from torch._guards import Source\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_guards.py\", line 14, in <module>\n",
            "    import sympy  # type: ignore[import]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/__init__.py\", line 73, in <module>\n",
            "    from .polys import (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/polys/__init__.py\", line 75, in <module>\n",
            "    from .polyfuncs import (symmetrize, horner, interpolate,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/polys/polyfuncs.py\", line 11, in <module>\n",
            "    from sympy.polys.specialpolys import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/polys/specialpolys.py\", line 297, in <module>\n",
            "    from sympy.polys.rings import ring\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/polys/rings.py\", line 30, in <module>\n",
            "    from sympy.printing.defaults import DefaultPrinting\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/printing/__init__.py\", line 5, in <module>\n",
            "    from .latex import latex, print_latex, multiline_latex\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/printing/latex.py\", line 18, in <module>\n",
            "    from sympy.tensor.array import NDimArray\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/tensor/__init__.py\", line 4, in <module>\n",
            "    from .indexed import IndexedBase, Idx, Indexed\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/tensor/indexed.py\", line 114, in <module>\n",
            "    from sympy.functions.special.tensor_functions import KroneckerDelta\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/functions/__init__.py\", line 37, in <module>\n",
            "    from sympy.functions.special.bsplines import bspline_basis, bspline_basis_set, interpolating_spline\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1012, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 672, in _compile_bytecode\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# training and testing\n",
        "%cd /home/novelty-detection-algorithms-evaluation\n",
        "!python generic_one_to_many.py --mode train --data MNIST --model RD4AD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nCn6ozguXqP",
        "outputId": "f68f15ad-c493-43c6-df9c-9d5baf675883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/anomalib/config/config.py:238: UserWarning: The seed value is now fixed to 0. Up to v0.3.7, the seed was not fixed when the seed value was set to 0. If you want to use the random seed, please select `None` for the seed value (`null` in the YAML file) or remove the `seed` key from the YAML file.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/anomalib/config/config.py:275: UserWarning: config.project.unique_dir is set to False. This does not ensure that your results will be written in an empty directory and you may overwrite files.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "FeatureExtractor is deprecated. Use TimmFeatureExtractor instead. Both FeatureExtractor and TimmFeatureExtractor will be removed in a future release.\n",
            "/usr/local/lib/python3.10/dist-packages/anomalib/utils/callbacks/__init__.py:142: UserWarning: Export option: None not found. Defaulting to no model export\n",
            "  warnings.warn(f\"Export option: {config.optimization.export_mode} not found. Defaulting to no model export\")\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:55: LightningDeprecationWarning: Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v2.0. Please pass `Trainer.fit(ckpt_path=)` directly instead.\n",
            "  rank_zero_deprecation(\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(limit_predict_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `ROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "2023-07-26 23:29:22.279856: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-26 23:29:22.332402: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-26 23:29:23.296545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Testing DataLoader 0: 100% 31/31 [00:10<00:00,  2.99it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m       image_AUPR        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7810227274894714    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8069707155227661    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.766698956489563    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "/usr/local/lib/python3.10/dist-packages/anomalib/config/config.py:275: UserWarning: config.project.unique_dir is set to False. This does not ensure that your results will be written in an empty directory and you may overwrite files.\n",
            "  warn(\n",
            "Global seed set to 1\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "FeatureExtractor is deprecated. Use TimmFeatureExtractor instead. Both FeatureExtractor and TimmFeatureExtractor will be removed in a future release.\n",
            "/usr/local/lib/python3.10/dist-packages/anomalib/utils/callbacks/__init__.py:142: UserWarning: Export option: None not found. Defaulting to no model export\n",
            "  warnings.warn(f\"Export option: {config.optimization.export_mode} not found. Defaulting to no model export\")\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:55: LightningDeprecationWarning: Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v2.0. Please pass `Trainer.fit(ckpt_path=)` directly instead.\n",
            "  rank_zero_deprecation(\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(limit_predict_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `ROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "2023-07-26 23:29:46.534730: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-07-26 23:29:46.585273: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-26 23:29:47.547059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Testing DataLoader 0: 100% 36/36 [00:12<00:00,  2.92it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m       image_AUPR        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9724795818328857    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m       image_AUROC       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9876061081886292    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m      image_F1Score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9703832864761353    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "/usr/local/lib/python3.10/dist-packages/anomalib/config/config.py:275: UserWarning: config.project.unique_dir is set to False. This does not ensure that your results will be written in an empty directory and you may overwrite files.\n",
            "  warn(\n",
            "Global seed set to 2\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "FeatureExtractor is deprecated. Use TimmFeatureExtractor instead. Both FeatureExtractor and TimmFeatureExtractor will be removed in a future release.\n"
          ]
        }
      ],
      "source": [
        "!python generic_one_to_many.py --mode test --data MNIST --model RD4AD"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "157foFEx7xywrDh--HOdSS0IiZJRYRCRN",
      "authorship_tag": "ABX9TyOhHiDB4wQxZoW17Exm8FTG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}